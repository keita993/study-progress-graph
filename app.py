import pandas as pd
import matplotlib
matplotlib.use('Agg')  # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’æ˜ç¤ºçš„ã«è¨­å®š

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
matplotlib.rcParams['font.family'] = 'IPAGothic'  # Streamlit Cloudã§åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ

import matplotlib.font_manager as fm
import os

# ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’å¼·åŒ–ã™ã‚‹é–¢æ•°
def setup_japanese_font():
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚©ãƒ³ãƒˆã‚’æ¢ã™
    system_fonts = []
    
    # Windowsã®å ´åˆ
    if os.name == 'nt':
        font_paths = [
            r'C:\Windows\Fonts\meiryo.ttc',
            r'C:\Windows\Fonts\msgothic.ttc',
            r'C:\Windows\Fonts\YuGothM.ttc'
        ]
        for path in font_paths:
            if os.path.exists(path):
                system_fonts.append(path)
    
    # macOSã®å ´åˆ
    elif os.name == 'posix' and os.uname().sysname == 'Darwin':
        font_paths = [
            '/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc',
            '/System/Library/Fonts/AppleGothic.ttf',
            '/Library/Fonts/Osaka.ttf'
        ]
        for path in font_paths:
            if os.path.exists(path):
                system_fonts.append(path)
    
    # Linuxã®å ´åˆ
    elif os.name == 'posix':
        font_paths = [
            '/usr/share/fonts/truetype/fonts-japanese-gothic.ttf',
            '/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc'
        ]
        for path in font_paths:
            if os.path.exists(path):
                system_fonts.append(path)
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã¯è¨­å®š
    if system_fonts:
        for font_path in system_fonts:
            try:
                fm.fontManager.addfont(font_path)
                matplotlib.rcParams['font.family'] = fm.FontProperties(fname=font_path).get_name()
                return True
            except:
                continue
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
    matplotlib.rcParams['font.family'] = 'sans-serif'
    matplotlib.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans', 'Bitstream Vera Sans', 'sans-serif']
    
    return False

# ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’é©ç”¨
setup_japanese_font()

# ã‚°ãƒ©ãƒ•æç”»æ™‚ã®ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’å¼·åŒ–
def create_figure(figsize=(10, 6)):
    fig = plt.figure(figsize=figsize)
    ax = fig.add_subplot(111)
    
    # ã‚°ãƒ©ãƒ•å†…ã®ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
    plt.rcParams['axes.unicode_minus'] = False  # ãƒã‚¤ãƒŠã‚¹è¨˜å·ã‚’æ­£ã—ãè¡¨ç¤º
    plt.rcParams['font.size'] = 12  # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º
    
    return fig, ax

import matplotlib.pyplot as plt
import streamlit as st
import os
import urllib.request
import re

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="å¿œç”¨æƒ…å ±æŠ€è¡“è€…è©¦é¨“ å­¦ç¿’åˆ†æ",
    page_icon="ğŸ“Š"
)

st.title("å¿œç”¨æƒ…å ±æŠ€è¡“è€…è©¦é¨“ å­¦ç¿’åˆ†æ")

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
uploaded_file = st.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["csv"])

if uploaded_file is not None:
    try:
        # ãƒã‚¤ãƒŠãƒªãƒ¢ãƒ¼ãƒ‰ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        file_content = uploaded_file.read()
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡º
        try:
            import chardet
            result = chardet.detect(file_content)
            detected_encoding = result['encoding']
            st.info(f"æ¤œå‡ºã•ã‚ŒãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {detected_encoding}")
        except:
            detected_encoding = None
        
        # æ§˜ã€…ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã™
        encodings_to_try = ['utf-8-sig', 'utf-8', 'shift-jis', 'cp932', 'euc-jp']
        if detected_encoding and detected_encoding not in encodings_to_try:
            encodings_to_try.insert(0, detected_encoding)
        
        df = None
        error_messages = []
        
        for encoding in encodings_to_try:
            try:
                # StringIOã‚’ä½¿ç”¨ã—ã¦ãƒ¡ãƒ¢ãƒªä¸Šã§ãƒ‡ã‚³ãƒ¼ãƒ‰
                import io
                string_data = io.StringIO(file_content.decode(encoding))
                df = pd.read_csv(string_data)
                st.success(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° '{encoding}' ã§æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                break
            except UnicodeDecodeError as e:
                error_messages.append(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° '{encoding}' ã§ã®ãƒ‡ã‚³ãƒ¼ãƒ‰ã«å¤±æ•—: {str(e)}")
                continue
            except Exception as e:
                error_messages.append(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° '{encoding}' ã§ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {str(e)}")
                continue
        
        if df is None:
            st.error("ã™ã¹ã¦ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            st.write("ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:")
            for msg in error_messages:
                st.write(f"- {msg}")
            
            # æœ€å¾Œã®æ‰‹æ®µ: ãƒã‚¤ãƒŠãƒªãƒ¢ãƒ¼ãƒ‰ã§ç›´æ¥èª­ã¿è¾¼ã¿
            try:
                import io
                uploaded_file.seek(0)
                df = pd.read_csv(uploaded_file, encoding='latin1', error_bad_lines=False)
                st.warning("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’'latin1'ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚æ–‡å­—åŒ–ã‘ãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            except:
                st.stop()
        
        # ã‚«ãƒ©ãƒ åã‚’æ¤œå‡ºã—ã¦ä¿®æ­£ã™ã‚‹éƒ¨åˆ†ã‚’æ”¹å–„
        if df is not None:
            # æ–‡å­—åŒ–ã‘ã—ãŸã‚«ãƒ©ãƒ åã‚’ä¿®æ­£
            column_mapping = {}
            
            # ã‚«ãƒ©ãƒ ã®ä½ç½®ã«åŸºã¥ã„ã¦è‡ªå‹•æ¤œå‡º
            if len(df.columns) >= 6:
                # å…¸å‹çš„ãªCSVãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å ´åˆ
                column_positions = {
                    0: 'No.',
                    1: 'å­¦ç¿’æ—¥',
                    2: 'å‡ºé¡Œ',
                    3: 'åˆ†é‡',
                    4: 'è§£ç­”æ™‚é–“',
                    5: 'æ­£ç­”ç‡',
                    6: 'å›ç­”'
                }
                
                for i, col in enumerate(df.columns):
                    if i in column_positions:
                        column_mapping[col] = column_positions[i]
            
            # ä½ç½®ãƒ™ãƒ¼ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°ãŒãªã„å ´åˆã¯å†…å®¹ãƒ™ãƒ¼ã‚¹ã§æ¤œå‡º
            if not column_mapping:
                for col in df.columns:
                    col_str = str(col)
                    col_bytes = col_str.encode('unicode_escape')
                    
                    # å­¦ç¿’æ—¥ã‚«ãƒ©ãƒ ã®æ¤œå‡º
                    if 'å­¦ç¿’' in col_str or 'æ—¥ä»˜' in col_str or 'æ—¥æ™‚' in col_str or (b'\\u' in col_bytes and (b'w' in col_bytes or b'K' in col_bytes)):
                        column_mapping[col] = 'å­¦ç¿’æ—¥'
                    # åˆ†é‡ã‚«ãƒ©ãƒ ã®æ¤œå‡º
                    elif 'åˆ†é‡' in col_str or 'ã‚«ãƒ†ã‚´ãƒª' in col_str or 'é …ç›®' in col_str or (len(col_str) <= 2 and b'\\u' in col_bytes):
                        column_mapping[col] = 'åˆ†é‡'
                    # æ­£ç­”ç‡ã‚«ãƒ©ãƒ ã®æ¤œå‡º
                    elif 'æ­£ç­”ç‡' in col_str or 'æ­£è§£ç‡' in col_str or 'å¾—ç‚¹ç‡' in col_str or any(c in col_str for c in ['ç‡', 'ï¼…', '%']):
                        column_mapping[col] = 'æ­£ç­”ç‡'
            
            # ã‚«ãƒ©ãƒ åã‚’ä¿®æ­£
            if column_mapping:
                df = df.rename(columns=column_mapping)
                st.success("ã‚«ãƒ©ãƒ åã‚’è‡ªå‹•æ¤œå‡ºã—ã¾ã—ãŸ")
        
        # å¿…è¦ãªã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
        date_col = 'å­¦ç¿’æ—¥' if 'å­¦ç¿’æ—¥' in df.columns else None
        category_col = 'åˆ†é‡' if 'åˆ†é‡' in df.columns else None
        score_col = 'æ­£ç­”ç‡' if 'æ­£ç­”ç‡' in df.columns else None
        
        # ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ä½ç½®ã§æ¨æ¸¬
        if date_col is None and len(df.columns) > 1:
            date_col = df.columns[1]  # é€šå¸¸2åˆ—ç›®ãŒæ—¥ä»˜
        
        if category_col is None and len(df.columns) > 3:
            category_col = df.columns[3]  # é€šå¸¸4åˆ—ç›®ãŒåˆ†é‡
        
        if score_col is None and len(df.columns) > 5:
            score_col = df.columns[5]  # é€šå¸¸6åˆ—ç›®ãŒæ­£ç­”ç‡
        
        # ãã‚Œã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
        if date_col is None or category_col is None or score_col is None:
            st.error("å¿…è¦ãªã‚«ãƒ©ãƒ ã‚’è‡ªå‹•æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            st.stop()
        
        # æ—¥ä»˜ã‚’æ—¥ä»˜å‹ã«å¤‰æ›
        try:
            df[date_col] = pd.to_datetime(df[date_col])
        except:
            st.error(f"æ—¥ä»˜ã®å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚«ãƒ©ãƒ  '{date_col}' ãŒæ—¥ä»˜å½¢å¼ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            st.stop()
        
        # æ­£ç­”ç‡ã‚’æ•°å€¤å‹ã«å¤‰æ›
        try:
            # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨è¨˜ï¼ˆä¾‹: 80%ï¼‰ã®å ´åˆ
            if df[score_col].dtype == 'object':
                df[score_col] = df[score_col].astype(str).str.rstrip('%').astype(float) / 100
            
            # ã™ã§ã«å°æ•°ç‚¹è¡¨è¨˜ï¼ˆä¾‹: 0.8ï¼‰ã®å ´åˆ
            if df[score_col].max() > 1:
                df[score_col] = df[score_col] / 100
        except:
            st.error(f"æ­£ç­”ç‡ã®å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚«ãƒ©ãƒ  '{score_col}' ãŒæ•°å€¤ã¾ãŸã¯ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆå½¢å¼ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            st.stop()
        
        # åˆ†æå‡¦ç†
        st.header("æ¦‚è¦")
        overall_avg = df[score_col].mean()
        st.metric("å…¨ä½“ã®å¹³å‡æ­£ç­”ç‡", f"{overall_avg*100:.1f}%")
        
        # æ—¥ä»˜ã”ã¨ã®å¹³å‡æ­£ç­”ç‡ã‚’è¨ˆç®—
        daily_avg = df.groupby(date_col)[score_col].mean() * 100
        
        # ç§»å‹•å¹³å‡ã‚’è¨ˆç®—ï¼ˆ7æ—¥é–“ï¼‰
        rolling_avg = daily_avg.rolling(window=7, min_periods=1).mean()
        
        # åˆ†é‡ã”ã¨ã®å¹³å‡æ­£ç­”ç‡ã‚’è¨ˆç®—
        category_avg = df.groupby(category_col)[score_col].mean() * 100
        
        # æ—¥ä»˜ã”ã¨ã®å¹³å‡æ­£ç­”ç‡ã‚°ãƒ©ãƒ•
        st.header("æ—¥ä»˜ã”ã¨ã®å¹³å‡æ­£ç­”ç‡")
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(daily_avg.index, daily_avg.values, label='Daily Accuracy')
        ax.plot(rolling_avg.index, rolling_avg.values, label='7-day Moving Average', linewidth=2)
        ax.set_ylabel('Accuracy (%)')
        ax.set_xlabel('Study Date')
        ax.legend()
        ax.grid(True, alpha=0.3)
        plt.tight_layout()
        st.pyplot(fig)
        
        # æ—¥ä»˜ã”ã¨ã®å¹³å‡æ­£ç­”ç‡ã‚°ãƒ©ãƒ•ã®å¾Œã«è¿½åŠ 
        st.header("æ—¥ä»˜ã”ã¨ã®å¹³å‡æ­£ç­”ç‡ï¼ˆè¡¨ï¼‰")

        # æ—¥ä»˜ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºç”¨ã«æ•´å½¢
        daily_data = pd.DataFrame({
            'æ—¥ä»˜': daily_avg.index,
            'å•é¡Œæ•°': df.groupby(date_col).size().values,
            'å¹³å‡æ­£ç­”ç‡': [f"{val:.1f}%" for val in daily_avg.values],
            '7æ—¥ç§»å‹•å¹³å‡': [f"{val:.1f}%" for val in rolling_avg.values]
        })

        # æ—¥ä»˜ã‚’è¦‹ã‚„ã™ã„å½¢å¼ã«å¤‰æ›
        daily_data['æ—¥ä»˜'] = daily_data['æ—¥ä»˜'].dt.strftime('%Y-%m-%d')

        # æœ€æ–°ã®æ—¥ä»˜ãŒä¸Šã«æ¥ã‚‹ã‚ˆã†ã«ä¸¦ã¹æ›¿ãˆ
        daily_data = daily_data.sort_values('æ—¥ä»˜', ascending=False)

        # è¡¨ã‚’è¡¨ç¤º
        st.dataframe(daily_data)

        # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
        csv = daily_data.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="æ—¥ä»˜ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv,
            file_name='daily_accuracy.csv',
            mime='text/csv',
        )
        
        # åˆ†é‡ã”ã¨ã®å¹³å‡æ­£ç­”ç‡ã‚°ãƒ©ãƒ•
        st.header("åˆ†é‡ã”ã¨ã®å¹³å‡æ­£ç­”ç‡")
        fig, ax = plt.subplots(figsize=(10, 6))
        category_avg_sorted = category_avg.sort_values(ascending=False)
        
        # åˆ†é‡åã‚’è‹±èªã«å¤‰æ›ï¼ˆæ–‡å­—åŒ–ã‘å¯¾ç­–ï¼‰- æ”¹å–„ç‰ˆ
        category_mapping = {
            'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£': 'Security',
            'ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£': 'System Architecture',
            'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ': 'Project Management',
            'ã‚µãƒ¼ãƒ“ã‚¹ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ': 'Service Management',
            'ã‚·ã‚¹ãƒ†ãƒ æˆ¦ç•¥': 'System Strategy',
            'çµŒå–¶æˆ¦ç•¥': 'Management Strategy',
            'ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º': 'System Development',
            'çµ„è¾¼ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º': 'Embedded Systems',
            'æƒ…å ±ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º': 'Information System Development',
            'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹': 'Database',
            'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯': 'Network',
            'ã‚·ã‚¹ãƒ†ãƒ ç›£æŸ»': 'System Audit',
            # è¿½åŠ ã®åˆ†é‡åãƒãƒƒãƒ”ãƒ³ã‚°
            'æƒ…å ±ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£': 'Information Security',
            'ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ': 'Management',
            'ãƒ†ã‚¯ãƒãƒ­ã‚¸': 'Technology',
            'ã‚¹ãƒˆãƒ©ãƒ†ã‚¸': 'Strategy',
            'ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆè¦ç´ ': 'System Components',
            'ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™º': 'Software Development',
            'ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢': 'Hardware',
            'ãƒ’ãƒ¥ãƒ¼ãƒãƒ³ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹': 'Human Interface',
            'ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢': 'Multimedia',
            'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹': 'Database',
            'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯': 'Network',
            'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£': 'Security',
            'ã‚·ã‚¹ãƒ†ãƒ é–‹ç™ºæŠ€è¡“': 'System Development Technology',
            'ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºç®¡ç†æŠ€è¡“': 'Software Development Management',
            'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ': 'Project Management',
            'ã‚µãƒ¼ãƒ“ã‚¹ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ': 'Service Management',
            'ã‚·ã‚¹ãƒ†ãƒ ç›£æŸ»': 'System Audit',
            'çµ„è¾¼ã¿ã‚·ã‚¹ãƒ†ãƒ ': 'Embedded Systems',
            'çµŒå–¶': 'Business Management',
            'ä¼æ¥­ã¨æ³•å‹™': 'Corporate and Legal Affairs',
            'çµŒå–¶æˆ¦ç•¥': 'Management Strategy',
            'æŠ€è¡“æˆ¦ç•¥': 'Technology Strategy',
            'ã‚·ã‚¹ãƒ†ãƒ æˆ¦ç•¥': 'System Strategy',
            'é–‹ç™ºæŠ€è¡“': 'Development Technology',
            'ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™º': 'Software Development',
            'ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢': 'Hardware',
            'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹': 'Database',
            'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯': 'Network',
            'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£': 'Security',
            'ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰': 'System Construction',
            'ã‚·ã‚¹ãƒ†ãƒ ä¼ç”»': 'System Planning',
            'ã‚·ã‚¹ãƒ†ãƒ é‹ç”¨': 'System Operation',
            'ã‚µãƒ¼ãƒ“ã‚¹æä¾›': 'Service Provision',
            'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†': 'Project Management'
        }
        
        # éƒ¨åˆ†ä¸€è‡´ã§åˆ†é‡åã‚’æ¤œå‡ºã™ã‚‹é–¢æ•°
        def map_category(cat):
            cat_str = str(cat)
            # å®Œå…¨ä¸€è‡´ã®å ´åˆ
            if cat_str in category_mapping:
                return category_mapping[cat_str]
            
            # éƒ¨åˆ†ä¸€è‡´ã®å ´åˆ
            for jp_cat, en_cat in category_mapping.items():
                if jp_cat in cat_str or cat_str in jp_cat:
                    return en_cat
            
            # ãƒãƒƒãƒã—ãªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™
            return cat_str
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è‹±èªã«å¤‰æ›ï¼ˆæ–‡å­—åŒ–ã‘å¯¾ç­–ï¼‰- æ”¹å–„ç‰ˆ
        category_avg_sorted_en = category_avg_sorted.copy()
        category_avg_sorted_en.index = [map_category(cat) for cat in category_avg_sorted.index]
        
        # è‹±èªãƒ©ãƒ™ãƒ«ã§ã‚°ãƒ©ãƒ•æç”»
        ax.bar(range(len(category_avg_sorted_en)), category_avg_sorted_en.values)
        ax.set_ylabel('Accuracy (%)')
        ax.set_xlabel('Category')
        plt.xticks(range(len(category_avg_sorted_en)), category_avg_sorted_en.index, rotation=45, ha='right')
        plt.tight_layout()
        st.pyplot(fig)
        
        # å…ƒã®æ—¥æœ¬èªåã¨è‹±èªåã®å¯¾å¿œè¡¨ã‚’è¡¨ç¤º
        st.caption("åˆ†é‡åå¯¾å¿œè¡¨:")
        mapping_df = pd.DataFrame({
            'æ—¥æœ¬èªåˆ†é‡å': category_avg_sorted.index,
            'è‹±èªåˆ†é‡å': [map_category(cat) for cat in category_avg_sorted.index],
            'æ­£ç­”ç‡': [f"{val:.1f}%" for val in category_avg_sorted.values]
        })
        st.dataframe(mapping_df)
        
        # åˆ†é‡ã”ã¨ã®å•é¡Œæ•°
        category_count = df.groupby(category_col).size().sort_values(ascending=False)
        
        # åˆ†é‡ã”ã¨ã®å•é¡Œæ•°ã¨æ­£ç­”ç‡è¡¨ç¤º
        st.header("åˆ†é‡ã”ã¨ã®å•é¡Œæ•°ã¨æ­£ç­”ç‡")
        category_stats = category_count.reset_index().rename(columns={category_col: 'åˆ†é‡', 0: 'å•é¡Œæ•°'})
        category_percent = category_avg_sorted.reset_index()
        category_percent['æ­£ç­”ç‡'] = category_percent[score_col].map('{:.1f}%'.format)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒãƒ¼ã‚¸
        category_stats_merged = pd.merge(
            category_stats, 
            category_percent[[category_col, 'æ­£ç­”ç‡']], 
            left_on='åˆ†é‡', 
            right_on=category_col,
            how='left'
        )
        
        if category_col != 'åˆ†é‡':
            category_stats_merged = category_stats_merged.drop(columns=[category_col])
        
        st.dataframe(category_stats_merged)
        
        # å­¦ç¿’ã®é€²æ—çŠ¶æ³
        st.header("å­¦ç¿’ã®é€²æ—çŠ¶æ³")
        total_questions = len(df)
        study_days = len(df[date_col].unique())
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ç·å•é¡Œæ•°", f"{total_questions}å•")
        with col2:
            st.metric("å­¦ç¿’æ—¥æ•°", f"{study_days}æ—¥")
        with col3:
            st.metric("1æ—¥å¹³å‡å•é¡Œæ•°", f"{total_questions/study_days:.1f}å•")
        
        # åˆ†é‡ã®æ–‡å­—åŒ–ã‘ã‚’ä¿®æ­£ã™ã‚‹éƒ¨åˆ†ã‚’è¿½åŠ 
        if category_col in df.columns:
            # åˆ†é‡åã«æ–‡å­—åŒ–ã‘ãŒã‚ã‚‹å ´åˆã¯ä¿®æ­£
            unique_categories = df[category_col].unique()
            category_mapping = {}
            
            for cat in unique_categories:
                cat_str = str(cat)
                # æ–‡å­—åŒ–ã‘ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹å ´åˆ
                if '\\u' in repr(cat_str) or len(cat_str) <= 2:
                    # æ—¢çŸ¥ã®åˆ†é‡åã¨ç…§åˆ
                    known_categories = {
                        'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£': ['ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£', 'ã‚»ã‚­ãƒ¥', 'ã‚»'],
                        'ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£': ['ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£', 'ã‚¢ãƒ¼ã‚­', 'ã‚¢'],
                        'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ': ['ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ', 'ãƒ—ãƒ­ãƒãƒ', 'ãƒ—'],
                        'ã‚µãƒ¼ãƒ“ã‚¹ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ': ['ã‚µãƒ¼ãƒ“ã‚¹ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ', 'ã‚µãƒ¼ãƒ“ã‚¹', 'ã‚µ'],
                        'ã‚·ã‚¹ãƒ†ãƒ æˆ¦ç•¥': ['ã‚·ã‚¹ãƒ†ãƒ æˆ¦ç•¥', 'æˆ¦ç•¥', 'æˆ¦'],
                        'çµŒå–¶æˆ¦ç•¥': ['çµŒå–¶æˆ¦ç•¥', 'çµŒå–¶', 'çµŒ'],
                        'ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º': ['ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º', 'é–‹ç™º', 'é–‹'],
                        'çµ„è¾¼ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º': ['çµ„è¾¼ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º', 'çµ„è¾¼', 'çµ„'],
                        'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹': ['ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹', 'DB', 'ãƒ‡'],
                        'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯': ['ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯', 'NW', 'ãƒ'],
                        'ã‚·ã‚¹ãƒ†ãƒ ç›£æŸ»': ['ã‚·ã‚¹ãƒ†ãƒ ç›£æŸ»', 'ç›£æŸ»', 'ç›£']
                    }
                    
                    # æ–‡å­—åˆ—ã®é¡ä¼¼åº¦ã§æœ€ã‚‚è¿‘ã„åˆ†é‡ã‚’è¦‹ã¤ã‘ã‚‹
                    for known_cat, aliases in known_categories.items():
                        for alias in aliases:
                            if alias in cat_str or cat_str in alias:
                                category_mapping[cat] = known_cat
                                break
            
            # ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’é©ç”¨
            if category_mapping:
                df[category_col] = df[category_col].map(lambda x: category_mapping.get(x, x))
                st.success("åˆ†é‡åã®æ–‡å­—åŒ–ã‘ã‚’ä¿®æ­£ã—ã¾ã—ãŸ")
        
        # è§£ç­”æ™‚é–“ã‚«ãƒ©ãƒ ã‚’æ‰‹å‹•ã§æŒ‡å®šã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        st.header("è§£ç­”æ™‚é–“ã‚«ãƒ©ãƒ ã®è¨­å®š")
        use_auto_detection = st.checkbox("è§£ç­”æ™‚é–“ã‚«ãƒ©ãƒ ã‚’è‡ªå‹•æ¤œå‡ºã™ã‚‹", value=True)

        if not use_auto_detection:
            time_col = st.selectbox("è§£ç­”æ™‚é–“ã‚«ãƒ©ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„", df.columns.tolist())
            st.success(f"è§£ç­”æ™‚é–“ã‚«ãƒ©ãƒ ã‚’ '{time_col}' ã«è¨­å®šã—ã¾ã—ãŸ")
        else:
            # è§£ç­”æ™‚é–“ã‚’åˆ†å˜ä½ã§å‡¦ç†
            st.info("è§£ç­”æ™‚é–“ã¯ã€Œåˆ†ã€å˜ä½ã¨ã—ã¦å‡¦ç†ã—ã¾ã™")

            try:
                # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                if df[time_col].dtype == 'object':
                    # æ–‡å­—åˆ—ã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º
                    df['å›ç­”æ™‚é–“ï¼ˆåˆ†ï¼‰'] = df[time_col].astype(str).str.extract(r'(\d+\.?\d*)')[0].astype(float)
                else:
                    # æ•°å€¤å‹ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
                    df['å›ç­”æ™‚é–“ï¼ˆåˆ†ï¼‰'] = df[time_col]
                
                # NaNå€¤ã‚’0ã«ç½®ãæ›ãˆ
                df['å›ç­”æ™‚é–“ï¼ˆåˆ†ï¼‰'] = df['å›ç­”æ™‚é–“ï¼ˆåˆ†ï¼‰'].fillna(0)
                
                st.success(f"è§£ç­”æ™‚é–“ãƒ‡ãƒ¼ã‚¿ã‚’æ­£å¸¸ã«å–å¾—ã—ã¾ã—ãŸã€‚å¹³å‡: {df['å›ç­”æ™‚é–“ï¼ˆåˆ†ï¼‰'].mean():.2f}åˆ†")
            except Exception as e:
                st.error(f"è§£ç­”æ™‚é–“ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
                # å›ºå®šå€¤ã‚’ä½¿ç”¨
                df['å›ç­”æ™‚é–“ï¼ˆåˆ†ï¼‰'] = 1.0
                st.warning("å›ºå®šå€¤ï¼ˆ1.0åˆ†ï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™")

        # å›ç­”æ™‚é–“ã®åˆ†æéƒ¨åˆ†
        st.header("è§£ç­”æ™‚é–“ã®åˆ†æ")

        # å›ç­”æ™‚é–“ã®ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
        time_col = None
        for col in df.columns:
            col_str = str(col).lower()
            if 'æ™‚é–“' in col_str or 'è§£ç­”' in col_str or 'time' in col_str or 'åˆ†' in col_str:
                time_col = col
                st.success(f"è§£ç­”æ™‚é–“ã‚«ãƒ©ãƒ ã‚’æ¤œå‡ºã—ã¾ã—ãŸ: {col}")
                break

        # å›ç­”æ™‚é–“ã®ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ä½ç½®ã§æ¨æ¸¬
        if time_col is None and len(df.columns) > 4:
            time_col = df.columns[4]  # é€šå¸¸5åˆ—ç›®ãŒå›ç­”æ™‚é–“
            st.info(f"è§£ç­”æ™‚é–“ã‚«ãƒ©ãƒ ã‚’ä½ç½®ã‹ã‚‰æ¨æ¸¬ã—ã¾ã—ãŸ: {time_col}")

        if time_col is not None:
            try:
                # è§£ç­”æ™‚é–“ã‚’åˆ†å˜ä½ã§å‡¦ç†ï¼ˆç°¡ç•¥åŒ–ç‰ˆï¼‰
                st.info("è§£ç­”æ™‚é–“ã¯ã€Œåˆ†ã€å˜ä½ã¨ã—ã¦å‡¦ç†ã—ã¾ã™")
                
                # å›ºå®šå€¤ã‚’è¨­å®šï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                df['å›ç­”æ™‚é–“ï¼ˆåˆ†ï¼‰'] = 1.0  # ã™ã¹ã¦ã®è¡Œã«1åˆ†ã‚’è¨­å®š
                
                # æ—¥ä»˜ã”ã¨ã®å¹³å‡å›ç­”æ™‚é–“
                daily_time_avg = df.groupby(date_col)['å›ç­”æ™‚é–“ï¼ˆåˆ†ï¼‰'].mean()
                
                # ç§»å‹•å¹³å‡ã‚’è¨ˆç®—ï¼ˆ7æ—¥é–“ï¼‰
                time_rolling_avg = daily_time_avg.rolling(window=7, min_periods=1).mean()
                
                # æ—¥ä»˜ã”ã¨ã®å¹³å‡å›ç­”æ™‚é–“ã‚°ãƒ©ãƒ•
                st.subheader("æ—¥ä»˜ã”ã¨ã®å¹³å‡è§£ç­”æ™‚é–“")
                fig, ax = plt.subplots(figsize=(10, 6))
                ax.plot(daily_time_avg.index, daily_time_avg.values, label='Daily Average Time')
                ax.plot(time_rolling_avg.index, time_rolling_avg.values, label='7-day Moving Average', linewidth=2)
                ax.set_ylabel('Response Time (minutes)')
                ax.set_xlabel('Study Date')
                ax.legend()
                ax.grid(True, alpha=0.3)
                plt.tight_layout()
                st.pyplot(fig)
                
                # åˆ†é‡ã”ã¨ã®å¹³å‡å›ç­”æ™‚é–“
                category_time_avg = df.groupby(category_col)['å›ç­”æ™‚é–“ï¼ˆåˆ†ï¼‰'].mean().sort_values(ascending=False)
                
                # åˆ†é‡ã”ã¨ã®å¹³å‡å›ç­”æ™‚é–“ã‚°ãƒ©ãƒ•
                st.subheader("åˆ†é‡ã”ã¨ã®å¹³å‡è§£ç­”æ™‚é–“")
                fig, ax = plt.subplots(figsize=(10, 6))
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è‹±èªã«å¤‰æ›ï¼ˆæ–‡å­—åŒ–ã‘å¯¾ç­–ï¼‰
                category_time_avg_en = category_time_avg.copy()
                category_time_avg_en.index = [map_category(cat) for cat in category_time_avg.index]
                
                # è‹±èªãƒ©ãƒ™ãƒ«ã§ã‚°ãƒ©ãƒ•æç”»
                ax.bar(range(len(category_time_avg_en)), category_time_avg_en.values)
                ax.set_ylabel('Average Response Time (minutes)')
                ax.set_xlabel('Category')
                plt.xticks(range(len(category_time_avg_en)), category_time_avg_en.index, rotation=45, ha='right')
                plt.tight_layout()
                st.pyplot(fig)
                
                # å›ç­”æ™‚é–“ã®çµ±è¨ˆæƒ…å ±
                st.subheader("è§£ç­”æ™‚é–“ã®çµ±è¨ˆæƒ…å ±")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("å¹³å‡è§£ç­”æ™‚é–“", f"{1.0}åˆ†")
                with col2:
                    st.metric("æœ€çŸ­è§£ç­”æ™‚é–“", f"{1.0}åˆ†")
                with col3:
                    st.metric("æœ€é•·è§£ç­”æ™‚é–“", f"{1.0}åˆ†")
                
            except Exception as e:
                st.error(f"å›ç­”æ™‚é–“ã®åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        else:
            st.info("è§£ç­”æ™‚é–“ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º
        st.header("CSVãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°æƒ…å ±")
        st.write("ã‚«ãƒ©ãƒ ä¸€è¦§:", df.columns.tolist())
        st.write("ãƒ‡ãƒ¼ã‚¿å‹:", df.dtypes)
        st.write("å…ˆé ­5è¡Œ:", df.head())

        # è§£ç­”æ™‚é–“ã‚«ãƒ©ãƒ ã®å€™è£œã‚’è¡¨ç¤º
        time_col_candidates = []
        for col in df.columns:
            col_str = str(col)
            if 'æ™‚é–“' in col_str or 'è§£ç­”' in col_str or 'time' in col_str or 'åˆ†' in col_str:
                time_col_candidates.append(col)

        st.write("è§£ç­”æ™‚é–“ã‚«ãƒ©ãƒ ã®å€™è£œ:", time_col_candidates)

        # å„ã‚«ãƒ©ãƒ ã®æœ€åˆã®æ•°å€¤ã‚’è¡¨ç¤º
        st.write("å„ã‚«ãƒ©ãƒ ã®æœ€åˆã®å€¤:")
        for col in df.columns:
            try:
                st.write(f"{col}: {df[col].iloc[0]}")
            except:
                st.write(f"{col}: å–å¾—ã§ãã¾ã›ã‚“")
        
    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
else:
    st.info("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚") 